{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, Dropdown\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/75/ql8v_2rx6v31k_cvbx9k4t500000gn/T/ipykernel_60437/2401159349.py:8: DtypeWarning: Columns (963) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(data_path, sep=';')\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "asset_number = 16\n",
    "data_path = './by_asset/C/assets/%s.csv' % asset_number\n",
    "events_path = './by_asset/C/assets/%s_events.csv' % asset_number\n",
    "feature_description_path = './CARE_To_Compare/Wind Farm C/feature_description.csv'\n",
    "\n",
    "# Read the sensor data\n",
    "data = pd.read_csv(data_path, sep=';')\n",
    "\n",
    "# Read the event data\n",
    "events = pd.read_csv(events_path, sep=';')\n",
    "\n",
    "# Read the feature description data\n",
    "feature_description = pd.read_csv(feature_description_path, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab5ba0a83a240e7a6cd61d9a1e14a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Select Sensor:', options={'sensor_0: ABB-LS Input K1, IL1 [A] (ave…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_sensor(sensor_id)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure feature_description contains the required columns\n",
    "required_columns = {'sensor_name', 'description', 'unit'}\n",
    "if not required_columns.issubset(feature_description.columns):\n",
    "    raise ValueError(f\"The feature description file must contain the columns: {required_columns}\")\n",
    "\n",
    "# Convert time columns to datetime for proper comparison\n",
    "data['time_stamp'] = pd.to_datetime(data['time_stamp'])\n",
    "events['event_start'] = pd.to_datetime(events['event_start'])\n",
    "events['event_end'] = pd.to_datetime(events['event_end'])\n",
    "\n",
    "# Initialize the event column in data\n",
    "data['event'] = 'other'\n",
    "\n",
    "# Assign event labels to the data based on the event time ranges\n",
    "for _, event in events.iterrows():\n",
    "    mask = (data['time_stamp'] >= event['event_start']) & (data['time_stamp'] <= event['event_end'])\n",
    "    data.loc[mask, 'event'] = event['event_label']\n",
    "\n",
    "# Define shorthand to longhand mapping\n",
    "shorthand_to_longhand = {\n",
    "    '_avg': 'average',\n",
    "    '_max': 'maximum',\n",
    "    '_min': 'minimum',\n",
    "    '_std': 'std_dev'\n",
    "}\n",
    "\n",
    "# Create a mapping of sensor names to their descriptions and units\n",
    "sensor_mapping = {}\n",
    "for _, row in feature_description.iterrows():\n",
    "    base_sensor = row['sensor_name']\n",
    "    description = row['description']\n",
    "    unit = row['unit']\n",
    "    for shorthand, longhand in shorthand_to_longhand.items():\n",
    "        full_sensor_name = f\"{base_sensor}{shorthand}\"\n",
    "        full_description = f\"{base_sensor}: {description} [{unit}] ({longhand})\"\n",
    "        sensor_mapping[full_sensor_name] = full_description\n",
    "\n",
    "# Filter the sensor columns and map their names to descriptions\n",
    "sensor_columns = [col for col in data.columns if col.startswith('sensor_') or col.startswith('power_') or col.startswith('wind_speed_')]\n",
    "sensor_options = {sensor_mapping[sensor]: sensor for sensor in sensor_columns if sensor in sensor_mapping}\n",
    "\n",
    "# Ensure the dropdown is not empty\n",
    "if not sensor_options:\n",
    "    raise ValueError(\"No matching sensors found between the data and the feature description file.\")\n",
    "\n",
    "# Function to plot selected sensor data with event-based coloring\n",
    "def plot_sensor(sensor_id):\n",
    "    if sensor_id not in data.columns:\n",
    "        raise ValueError(f\"Sensor '{sensor_id}' not found in the data.\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Filter data based on event type\n",
    "    anomaly = data[data['event'] == 'anomaly']\n",
    "    normal = data[data['event'] == 'normal']\n",
    "    other = data[data['event'] == 'other']\n",
    "    \n",
    "    # Plot data with different colors, ensuring red and green are on top\n",
    "    plt.plot(other['time_stamp'], other[sensor_id], 'b.', label='Other', alpha=0.5)\n",
    "    plt.plot(normal['time_stamp'], normal[sensor_id], 'g.', label='Normal', alpha=0.8)\n",
    "    plt.plot(anomaly['time_stamp'], anomaly[sensor_id], 'r.', label='Anomaly', alpha=0.8)\n",
    "    \n",
    "    plt.title(f'Plot of {sensor_mapping[sensor_id]}')\n",
    "    plt.xlabel('Time Stamp')\n",
    "    plt.ylabel(sensor_mapping[sensor_id])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Create a dropdown for sensor selection\n",
    "interact(plot_sensor, sensor_id=Dropdown(options=sensor_options, description='Select Sensor:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/75/ql8v_2rx6v31k_cvbx9k4t500000gn/T/ipykernel_60437/291792241.py:12: DtypeWarning: Columns (963) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(data_path, sep=';')\n",
      "/Users/osma/ARotor/.conda/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [14:19:24] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.37      0.88      0.52      2033\n",
      "         1.0       0.39      0.05      0.09      3234\n",
      "\n",
      "    accuracy                           0.37      5267\n",
      "   macro avg       0.38      0.46      0.30      5267\n",
      "weighted avg       0.38      0.37      0.25      5267\n",
      "\n",
      "Train timestamps: [Timestamp('2024-05-09 06:50:00'), Timestamp('2024-05-09 07:00:00'), Timestamp('2024-05-09 07:10:00'), Timestamp('2024-05-09 07:20:00'), Timestamp('2024-05-09 07:30:00')]\n",
      "Test timestamps: [Timestamp('2019-11-06 04:20:00'), Timestamp('2019-11-06 04:30:00'), Timestamp('2019-11-06 04:40:00'), Timestamp('2019-11-06 04:50:00'), Timestamp('2019-11-06 05:00:00')]\n"
     ]
    }
   ],
   "source": [
    "# Configurable variables\n",
    "sequence_length = 30  # Number of consecutive datapoints\n",
    "train_event_ids = [79, 46]  # Event IDs for training\n",
    "test_event_ids = [30, 65]  # Event IDs for testing\n",
    "time_step = timedelta(minutes=10)  # Expected time difference between consecutive rows\n",
    "\n",
    "# Load the data\n",
    "asset_number = 16\n",
    "data_path = './by_asset/C/assets/%s.csv' % asset_number\n",
    "events_path = './by_asset/C/assets/%s_events.csv' % asset_number\n",
    "\n",
    "data = pd.read_csv(data_path, sep=';')\n",
    "events = pd.read_csv(events_path, sep=';')\n",
    "\n",
    "# Convert time columns to datetime\n",
    "data['time_stamp'] = pd.to_datetime(data['time_stamp'])\n",
    "events['event_start'] = pd.to_datetime(events['event_start'])\n",
    "events['event_end'] = pd.to_datetime(events['event_end'])\n",
    "\n",
    "# Assign event labels to the data\n",
    "data['event'] = 'other'\n",
    "for _, event in events.iterrows():\n",
    "    mask = (data['time_stamp'] >= event['event_start']) & (data['time_stamp'] <= event['event_end'])\n",
    "    data.loc[mask, 'event'] = event['event_label']\n",
    "\n",
    "# Map event labels to binary classes\n",
    "data['event_class'] = data['event'].apply(lambda x: 1 if x == 'anomaly' else 0)\n",
    "\n",
    "\n",
    "# Function to prepare data for training/testing based on event IDs\n",
    "def prepare_data(data, events, event_ids, sequence_length, time_step):\n",
    "    filtered_data = pd.DataFrame()\n",
    "    for event_id in event_ids:\n",
    "        event = events[events['event_id'] == event_id]\n",
    "        for _, row in event.iterrows():\n",
    "            event_data = data[(data['time_stamp'] >= row['event_start']) & (data['time_stamp'] <= row['event_end'])]\n",
    "            filtered_data = pd.concat([filtered_data, event_data])\n",
    "    \n",
    "    # Sort by timestamp to ensure proper sequencing\n",
    "    filtered_data = filtered_data.sort_values(by='time_stamp')\n",
    "    return create_sequences(filtered_data, sequence_length, time_step)\n",
    "\n",
    "\n",
    "# Function to create sequences of 30 consecutive datapoints\n",
    "def create_sequences(data, sequence_length, time_step):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    timestamps = []\n",
    "    \n",
    "    # Identify gaps in time\n",
    "    data['time_diff'] = data['time_stamp'].diff().fillna(pd.Timedelta(seconds=0))\n",
    "    data['is_continuous'] = data['time_diff'] <= time_step\n",
    "    \n",
    "    # Reset the index whenever there is a gap\n",
    "    data['group'] = (~data['is_continuous']).cumsum()\n",
    "    \n",
    "    # Select only numeric columns for the model\n",
    "    numeric_columns = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_columns = [col for col in numeric_columns if col not in ['event_class', 'time_diff', 'is_continuous', 'group']]\n",
    "    \n",
    "    # Process each group separately\n",
    "    for _, group_data in data.groupby('group'):\n",
    "        group_data = group_data.reset_index(drop=True)\n",
    "        for i in range(len(group_data) - sequence_length + 1):\n",
    "            sequence = group_data.iloc[i:i + sequence_length]\n",
    "            sequences.append(sequence[numeric_columns].values.flatten())  # Use only numeric columns\n",
    "            labels.append(sequence['event_class'].mean())\n",
    "            timestamps.append(sequence['time_stamp'].iloc[0])  # Keep track of the start time of the sequence\n",
    "    \n",
    "    return np.array(sequences), np.array(labels), timestamps\n",
    "\n",
    "# Prepare training and testing data\n",
    "X_train, y_train, train_timestamps = prepare_data(data, events, train_event_ids, sequence_length, time_step)\n",
    "X_test, y_test, test_timestamps = prepare_data(data, events, test_event_ids, sequence_length, time_step)\n",
    "\n",
    "# Train the XGBoost model\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Example output of timestamps for debugging\n",
    "print(\"Train timestamps:\", train_timestamps[:5])\n",
    "print(\"Test timestamps:\", test_timestamps[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.8-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/osma/ARotor/.conda/lib/python3.11/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/osma/ARotor/.conda/lib/python3.11/site-packages (from ipywidgets) (8.26.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/osma/ARotor/.conda/lib/python3.11/site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.15-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.16-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: decorator in /Users/osma/ARotor/.conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/osma/ARotor/.conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/osma/ARotor/.conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/osma/ARotor/.conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/osma/ARotor/.conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /Users/osma/ARotor/.conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /Users/osma/ARotor/.conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/osma/ARotor/.conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: wcwidth in /Users/osma/ARotor/.conda/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/osma/ARotor/.conda/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/osma/ARotor/.conda/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/osma/ARotor/.conda/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/osma/ARotor/.conda/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /Users/osma/ARotor/.conda/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/osma/ARotor/.conda/lib/python3.11/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Downloading ipywidgets-8.1.8-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.16-py3-none-any.whl (914 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m914.9/914.9 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-4.0.15-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [ipywidgets]3\u001b[0m [ipywidgets]\n",
      "\u001b[1A\u001b[2KSuccessfully installed ipywidgets-8.1.8 jupyterlab_widgets-3.0.16 widgetsnbextension-4.0.15\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install ipywidgets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
